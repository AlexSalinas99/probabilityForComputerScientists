
% rebase('templates/chapter.html', title="Binomial")
 
<center><h1>Binomial Distribution</h1></center>
<hr/>

<p>Consider $n$ independent trials of an experiment, where each trial is a "success" probability $p$. Let $X$ be number of successes in $n$ trials. This situation is truly common in the natural world, and as such there has been a lot of research into such phenomena. The name for random variables like $X$ is a Binomial Random variable. If you can identify that a process fits this discription, you can inherit many already proved properties, such as the PMF formula, expectation and variance!
</p>


<p>
<center>
<img src="{{pathToRoot}}img/chapters/binEquation.png" class="mainFigure"></img>
</center>
</p>

<p>Here are a few examples of Binomial random variables:
<ul>
	<li># of heads in $n$ coin flips
<li># of 1â€™s in randomly generated length $n$ bit string
<li># of disk drives crashed in 1000 computer cluster, assuming disks crash independently

</ul>


<%
  include('templates/rvCards/binomial.html')
%>
<div style="height: 20px"></div>

<p>One way to think of the Binomial is as the sum of $n$ <a href="{{pathToLang}}part2/bernoulli">Bernoilli</a> variables. Say that $Y_i \sim \Ber(p)$ is an indicator bernoulli variable which is 1 if experiment $i$ is a success. Then if $X$ is the total number of successes in $n$ experiments, $X \sim \Bin(n, p)$:
	$$
	X = \sum_{i=1}^n Y_i
	$$
<p>Recall that $Y_i$ will be 1 or 0 and as such one way to think of $X$ is the sum of those 1s and 0s.
<h2>Binomial PMF</h2>
<p>The most important property to know about a Binomial is its <a href="{{pathToLang}}part2/pmf">PMF function</a>:
<p>
<center>
<img src="{{pathToRoot}}img/chapters/binEquation2.png" class="mainFigure"></img>
</center>
</p>

<p>The full derivation of this formula is in fact something which we went over in part 1. There is a compelte example on the probability of $k$ heads in $n$ coin flips, where each flip is heads with probability $0.5$: <a href="{{pathToLang}}examples/many_flips">Many Coin Flips</a>. Briefly, if you think of each experiment as being distinct, then there are ${n \choose k}$ ways of permuting $k$ successes from $n$ experiments. For any of the mututally exclusive permutations, the probability of that permutation is $p^k \cdot (1-p)^{n-k}$.</p>

<p>The name binomial comes from the term ${n \choose k}$ which is formally called the binomial coefficient.</p>

<h2>Expectation of Binomial</h2>

<p>There is an easy way to calculate the expectation of a binomial, and a hard way. 

The easy way is to leverage the fact that a Binomial is the sum of Bernoulli random variables. $X = \sum_{i=1}^{n} Y_i$ where $Y_i \sim \Ber(p)$. Since the <a href="{{pathToLang}}part2/expectation">expectation of the sum</a> of random variables is the sum of expectations we can add the expectation, $\E[Y_i] = p$, of each of the Bernoulli's :
$$
\begin{align}
\E[X] &= \E\Big[\sum_{i=1}^{n} Y_i\Big] && \text{Since }X = \sum_{i=1}^{n} Y_i \\
&=  \sum_{i=1}^{n}\E[ Y_i] && \text{Expectation of sum} \\
&=  \sum_{i=1}^{n}p && \text{Expectation of Bernoulli} \\
&=  n \cdot p && \text{Sum $n$ times}
\end{align}
$$

	The hard way is to use the definition of expectation:
	$$
	\begin{align}
	\E[X] &= \sum_{i=0}^n i \cdot \p(X = i) && \text{Def of expectation} \\
	&= \sum_{i=0}^n i \cdot {n \choose i} p^i(1-p)^{n-i} && \text{Sub in PMF} \\
	& \cdots && \text{Many steps later} \\
	&= n \cdot p
	\end{align}
	$$


