% rebase('templates/chapter.html', title="Binomial")

<center><h1>Binomial Distribution</h1></center>
<hr/>

<p>In this section, we will discuss the binomial distribution. To start, imagine the following example. Consider $n$
    independent trials of an experiment where each trial is a "success" probability $p$. Let $X$ be the
    number of successes in $n$ trials. This situation is truly common in the natural world, and as such, there has been
    a lot of research into such phenomena. Random variables like $X$ are called binomial random variables. If you
    can identify that a process fits this description, you can inherit many already proved properties such as the PMF
    formula, expectation, and variance!
</p>


<p>
<center>
    <img src="{{pathToRoot}}img/chapters/binEquation.png" class="mainFigure"></img>
</center>
</p>

<p>Here are a few examples of binomial random variables:
<ul>
    <li># of heads in $n$ coin flips
    <li># of 1â€™s in randomly generated length $n$ bit string
    <li># of disk drives crashed in 1000 computer cluster, assuming disks crash independently
</ul>


<%
include('templates/rvCards/binomial.html')
%>
<div style="height: 20px"></div>

<!--What is an indicator r.v.?-->
<p>One way to think of the binomial is as the sum of $n$ <a href="{{pathToLang}}part2/bernoulli">Bernoulli</a>
    variables. Say that $Y_i \sim \Ber(p)$ is an indicator Bernoulli random variable which is 1 if experiment $i$ is a
    success. Then if $X$ is the total number of successes in $n$ experiments, $X \sim \Bin(n, p)$:
    $$
    X = \sum_{i=1}^n Y_i
    $$
<p>Recall that the outcome of $Y_i$ will be 1 or 0, so one way to think of $X$ is as the sum of those 1s and 0s.
<h2>Binomial PMF</h2>
<p>The most important property to know about a binomial is its <a href="{{pathToLang}}part2/pmf">PMF function</a>:
<p>
<center>
    <img src="{{pathToRoot}}img/chapters/binEquation2.png" class="mainFigure"></img>
</center>
</p>

<p>Recall, we derived this formula in Part 1. There is a complete example on the probability of $k$ heads in $n$ coin
    flips, where each flip is heads with probability $0.5$: <a
            href="{{pathToLang}}examples/many_flips">Many Coin Flips</a>. To briefly review, if you think of each
    experiment as
    being distinct, then there are ${n \choose k}$ ways of permuting $k$ successes from $n$ experiments. For any of the
    mutually exclusive permutations, the probability of that permutation is $p^k \cdot (1-p)^{n-k}$.</p>

<!--
Perhaps include a small explanation or link to the wiki that explains why this term is related to coefficients at all.
 -->
<p>The name binomial comes from the term ${n \choose k}$ which is formally called the binomial coefficient.</p>

<h2>Expectation of Binomial</h2>

<p>There is an easy way to calculate the expectation of a binomial and a hard way.

    The easy way is to leverage the fact that a binomial is the sum of Bernoulli random variables. $X = \sum_{i=1}^{n}
    Y_i$ where $Y_i \sim \Ber(p)$. Since the <a href="{{pathToLang}}part2/expectation">expectation of the sum</a> of
    random variables is the sum of expectations, we can add the expectation, $\E[Y_i] = p$, of each of the Bernoulli's:
    $$
    \begin{align}
    \E[X] &= \E\Big[\sum_{i=1}^{n} Y_i\Big] && \text{Since }X = \sum_{i=1}^{n} Y_i \\
    &= \sum_{i=1}^{n}\E[ Y_i] && \text{Expectation of sum} \\
    &= \sum_{i=1}^{n}p && \text{Expectation of Bernoulli} \\
    &= n \cdot p && \text{Sum $n$ times}
    \end{align}
    $$

    The hard way is to use the definition of expectation:
    $$
    \begin{align}
    \E[X] &= \sum_{i=0}^n i \cdot \p(X = i) && \text{Def of expectation} \\
    &= \sum_{i=0}^n i \cdot {n \choose i} p^i(1-p)^{n-i} && \text{Sub in PMF} \\
    & \cdots && \text{Many steps later} \\
    &= n \cdot p
    \end{align}
    $$


