
% rebase('templates/chapter.html', title="Likelihood Functions")
 
<center><h1>Probability Mass Functions</h1></center>
<hr/>

<p>For a random variable, the most important thing to know is: how likely is each outcome? For a discrete random variable, this information is called the  "<b>probability mass function</b>". The probability mass function (or PMF for short) provides the "mass" (aka amount) of "probability" for each possible assignment of the random variable. </p>

<p>Formally, the probability mass function (PMF) is a mapping between the values that the random variable could take on and the probability of the random variable taking on said value. In mathematics,
we call associations functions. There are many different ways of representing functions: you can write an equation, you can make a graph, you can even store many samples in a list. Lets start by looking at PMFs as graphs where the x-axis is the values
that the random variable could take on and the y-axis is the probability of the random variable taking on said
value.</p>

<p>In the following example, on the left we show a PMF, as a graph, for the random variable: $X$ = the value of a six sided die roll. On the right we show a constrasting example of a PMF for the ranom variable $X$ = value of the sum of two dice rolls:</p>

<p>
<div class="row">
	<div class="col-6">
		<img style="max-width:100%" src="{{pathToRoot}}img/chapters/dicePmf.png"></img>
	</div>
	<div class="col-6">
		<img style="max-width:100%" src="{{pathToRoot}}img/chapters/diceSumPmf.png"></img>
	</div>
</div>
<center><i>Left: the PMF of a single 6 sided die roll. Right: the PMF of the sum of two dice rolls.</i></center>
</p>

<p>
	The <a href="{{pathToLang}}part1/equally_likely/#sum_dice">sum of two dice example</a> in the equally likely probability section.  Again, the information that is provided in these graphs is the likelihood of a random variable taking on different values. In the graph on the right, the value "$6$" on the x-axis is associated with the prbability 5/36 on the y-axis. This x-axis refers to the event "the sum of two dice is 6", or $Y=6$. The y-axis is saying that the probability of that event is $5/36$. In full: $\p(Y=6) = 5/36$. The value "$2$" is associated with "1/36" which tells you that, $\p(Y=2) = 1/36$, the probability that two dice sum to 2 is 1/36. There is no value associated with "$1$" because the sum of two dice can not be 1. If you find this notation confusing, revisit the <a href="{{pathToLang}}part2/rvs/#rv_vs_event">random variables</a> section.
	</p>

	<p>
		Here is the exact same information in equation form.  :
		<div class="d-flex justify-content-around align-items-center">
			<div>
				$$
				\begin{align}
				\p(X=x) = â…™ && \text{if } 1 \leq x \leq 6
				\end{align}
				$$
			</div>
			<div>
				$$
				\p(Y=y) = \begin{cases}
					(y-1)/36 && \text{if } 1 \leq y \leq 7\\
					(13-y)/36 && \text{if } 8 \leq y \leq 12 
				\end{cases}
				$$
			</div>
		</div>
	</p>

	<p>
		As a final example, here is the PMF for $Y$, the sum of two dice, in python code:
		<pre class="language-python"><code>def pmf_sum_two_dice(y):
    # Returns the probability that the sum of two dice is y
    if y < 2 or y > 12:
        return 0
    if y <= 7:
        return (y-1) / 36
    else:
        return (13-y) / 36</code></pre></p>

<h2>Notation</h2>
<p>$\p(Y=y)$ might feel like redundant notation. In probability research papers, and higher level work, mathemeticians often use the shorthand with only the value. They would often write $\p(y)$ in place of $\p(Y=y)$. This shorthand assumes that the random variable referred to is the capital version of the value, and that the probability event is that the random variable equals the value $y$. In this book we will often use the full form of the event $\P(Y=y)$ but will occasionally use the shorthand $\p(y)$.</p>

<h2>Probabilities Must Sum to 1</h2>

For a variable (call it $X$) to be a proper random variable it must be the case that if you summed up the values of $\p(X = k)$ for all possible values $k$ that $X$ can take on, the result must be 1:
$$
\sum_{k} \p(X = k) = 1
$$
For further undertanding lets derive why this is the case. A random variable taking on a value is an event (for example $X = 2$). Each of those events are mutually exclusive because a random variable will take on exactly one value.



<h2 id="data_to_hist">Data to Histograms to Probability Mass Functions</h2>

<p>One suprising way to store a likelihood function (recall that a PMF is the name of the likelihood function for <i>discrete</i> random variables) is simply a list of data. We simulated summing two die 10,000 times to make this example dataset:</p>

% include('chapters/part2/pmf/sumDiceList.html')

<p>Note that this data, on its own, represents an approximation for the probability mass function. If you wanted to approximate $\p(Y=5)$ you could simply count the number of times that "5" occurs in your data. This is an approximation based on the <a href="">definition of probability</a>. Here is the full <a href="https://en.wikipedia.org/wiki/Histogram">histogram</a> of the data, a count of times each value occurs:</p>

<p>
	<center>
<img class="mainFigure" src="{{pathToRoot}}img/chapters/histogram.png"></src>
</center>
</p>

<p>A normalized histogram (where each value is divided by the length of your data list) is an approximation of the PMF. For a dataset of discrete numbers, a histogram shows the count of each value (in this case $y$). By the definition of probability, if you divide this count by the number of experiments run, you arrive at a approximation of the probability of the event $\p(Y=y)$. In our example, we have 10,000 elements in our dataset. The count of times that 3 occurs is 552. Note that:
	$$\begin{align}
	\frac{\text{count}(Y=5)}{n} &= \frac{552}{10000} = 0.0552 \\
	\p(Y=5) &= \frac{4}{36} = 0.0555
	\end{align}
	$$
</p>

<p>In this case, since we ran 10,000 trials, the histogram is a very good approximation of the PMF.

	We use sum of dice as an example because it is easy to understand. Datasets in the real world often represent more exciting events. 


