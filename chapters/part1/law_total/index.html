
% rebase('templates/chapter.html', title="Law of Total Probability")
 
<center><h1>Law of Total Probability</h1></center>
<hr/>

<p>An astute person once observed that when looking at a picture, like the one we say for conditional probability:
</p>

<p>
	<center>
<img src="{{pathToRoot}}img/chapters/conditionalProbability.png" class="mainFigure"></img>
</center>
</p>

<p>that event $E$ can be
thought of as having two parts, the part that is in $F$, $(E \and F)$, and the part that isnâ€™t, $(E \and F\c)$.
This is true
because $F$ and $F\c$ are (a) mutually exclusive sets of outcomes which (b) together cover the entire sample space.
After further investigation this proved to be mathematically true, and there was much rejoicing:</p>

$$\p(E) = \p(E \and F) + \p(E \and F\c)$$</p>

<p>This observation proved to be particularly useful when it was combined with the chain rule and gave rise to a
tool so useful, it was given the big name, law of total probability.</p>

<p>
	<div class="bordered">
		<b>The Law of Total Probability</b><br/>
If we combine our above observation with the chain rule, we get a very useful formula:
$$
\p(E) = \p(E | F) \p(F) + \p(E | F\c) \p(F\c)
$$

<p>There is a more general version of the rule. If you can divide your sample space into any number of
<a href="{{pathToLang}}part1/prob_or/">mutually exclusive</a> events: $B_1, B_2, \dots B_n$ such that every outcome in sample space fall into one of those
events, then:
$$
\begin{align}
\p(E) 
&= \sum_{i=1}^n \p(E \and B_i) && \text{Extension of our observation}\\
&= \sum_{i=1}^n \p(E | B_i) \p(B_i) && \text{Using chain rule on each term}
\end{align}
$$</p>
	</div>
	</p>
