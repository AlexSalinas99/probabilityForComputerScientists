
% rebase('templates/chapter.html', title="Probability of and")
 

<center><h1>Probability of <b>and</b></h1></center>
<hr/>

<p>The probability of the <b><i>and</i></b> of two events, say $E$ and $F$, written $\p(E \and F)$, is the probability of both events happening. You might see equivalent notations $\p(EF)$, $\p(E âˆ© F)$ and $\p(E,F)$ to mean the probability of and. How you calculate the probability of event $E$ and event $F$ happening
depends on whether or not the events are "independent". In the same way that mutual exclusion makes it easy to calculate the probability of the <b><i>or</i></b> of events, independence is a property that makes it easy to calculate the <b><i>and</i></b> of events.</p>     

<h2>Independent Events</h2>

<p>If events are <a href="{{pathToLang}}part1/independence"><b></i>independent</i></b></a> then calculating the probability of <b><i>and</i></b> becomes simple multiplication:

<p>
	<div class="bordered">
		<b><i>Definition</i></b>: Probability of <b><i>and</i></b> for independent events.<br/>

		<p>If two events: $E$, $F$ are independent then the probability of $E$ <b><i>and</i></b> $F$ occuring is:
			$$
			\p(E \and F) = \p(E) \cdot \p(F)
			$$
		</p> 
		<p>This property applies regardless of how the probabilities of $E$ and $F$ were calculated and
whether or not the events are mutually exclusive. </p>

<p> The independence principle extends to more than two
events. For $n$ events $E_1, E_2, \dots E_n$ that are <b><i>mutually</i></b> independent of one another -- the independence equation also holds for all subsets of the events.
$$
\p(E_1 \and E_2 \and \dots \and E_n) = \prod_{i=1}^n \p(E_i)
$$
</p>
<p></p>
	</div>
</p>

<p >
	We can prove this equation by combining the definition of conditional probability and the definition of independence.
	<div class="purpleBox">
	<p><b><i>Proof</i></b>: If $E$ is independent of $F$ then $\p(E \and F) = \p(E) \cdot \p(F)$</p>
	$$
	\begin{align}
	\p(E|F) &= \frac{\p(E \and F)}{\p(F)} && \text{Definition of }
	\href{ {{pathToLang}}part1/cond_prob/}{\text{conditional probability}} 
	\\
	\p(E) &=  \frac{\p(E \and F)}{\p(F)} && \text{Definition of }
	\href{ {{pathToLang}}part1/independence/}{\text{independence}} \\
	\p(E \and F) &= \p(E) \cdot \p(F) && \text{Rearranging terms}
	\end{align}
	$$
</div>
	</p>

<p>See the chapter on <a href="{{pathToLang}}part1/independence">independence</a> to learn about when you can assume that two events are independent</p>

<h2>Dependent Events</h2>

<p>Events which are not independent are called <b><i>dependent</i></b> events. How can you calculate the <b>and</b> of dependent events? If your events are mutually exclusive you might be able to use a technique called DeMorgan's law, which we cover in a latter chapter. For the probability of and in dependent events there is a direct formula called the <a href="conditional">chain rule</a>  which we cover in the next chapter: <a href="conditional">Conditional Probability</a>.</p>

</p>