
<!DOCTYPE html>
<html>

<head>
    

<title>Bayes&#039; Theorem</title>

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta http-equiv="content-type" content="text/html; charset=UTF8">

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>



<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="../../../css/style.css">
   

<!-- Java Script -->
<script src="../../../plugins/moment.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

<!-- Python highlighting -->
<script src="../../../plugins/prism/prism.js"></script>
<link href="../../../plugins/prism/prism.css" rel="stylesheet" />

<!-- Probability Packages -->
<script src="../../../js/probabilityUtils.js"></script>
<script src="../../../plugins/probability/gaussian.js"></script>
<script src="../../../plugins/color.js"></script>

<!-- font awesome -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.0/css/all.css" integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">

<!-- SWAL -->
<script src="https://cdn.jsdelivr.net/npm/sweetalert2@9"></script>


<!-- Stanford -->
<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,700' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Source+Serif+Pro:400,600,700' rel='stylesheet' type='text/css'>

<!-- Math Jax -->
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script> -->
<script type="text/javascript">
  MathJax = {
    config: ["MMLorHTML.js"],
    jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML"],
    extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js", "action.js"]
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    },
	"fast-preview": {
	  disabled: true
	}
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>



<script src="../../../plugins/math.min.js"></script>



    <!-- Popper.JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"></script>
    <!-- Bootstrap JS -->
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js" integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm" crossorigin="anonymous"></script>

 <!-- Scrollbar Custom CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/malihu-custom-scrollbar-plugin/3.1.5/jquery.mCustomScrollbar.min.css">
    <script src="../../../plugins/jquery.mCustomScrollbar.js"></script>

</head>


<!-- I declare a few math operators I often use in equations -->
<div style="display:none">
  $\DeclareMathOperator{\p}{Pr}$
  $\DeclareMathOperator{\c}{^C}$
  $\DeclareMathOperator{\or}{  or}$
  $\DeclareMathOperator{\and}{  and}$
</div>

<body class="greyBackground">


    <div class="wrapper">
        <!-- Sidebar  -->
<nav id="sidebar">
    <div class="sidebar-header">
        <a href="../../"><h3><i class="fas fa-seedling"></i> Coding Probability</h3></a>
    </div>

<!-- intro -->
<ul class="list-unstyled components">
<li>
<a href="../../intro/intro">Introduction</a>
<a href="../../intro/free_book">A Free Online Textbook</a>
<a href="../../intro/notation">Notation</a>
<a href="../../intro/python">Probability in Python</a>
</li>
</ul>

<!-- part1 -->
<ul class="list-unstyled components">
<li><p href="{pathToLang}part1">Part 1: Core Probability</p></li>
<li>
<a href="../../part1/counting">Counting</a>
<a href="../../part1/combinatorics">Combinatorics</a>
<a href="../../part1/probability">Definition of Probability</a>
<a href="../../part1/equally_likely">Equally Likely Outcomes</a>
<a href="../../part1/prob_or">Probability of or</a>
<a href="../../part1/prob_and">Probability of and</a>
<a href="../../part1/cond_prob">Conditional Probability</a>
<a href="../../part1/law_total">Law of Total Probability</a>
<a href="../../part1/bayes_theorem">Bayes' Theorem</a>
<a href="../../part1/random_computers">Randomness in Computers</a>
<a href="../../part1/log_probabilities">Log Probabilities</a>
</li>
</ul>

<!-- part2 -->
<ul class="list-unstyled components">
<li><p href="{pathToLang}part2">Part 2: Random Variables</p></li>
<li>
<a href="../../part2/rvs">Random Variables</a>
<a href="../../part2/pmf">Probability Mass Functions</a>
<a href="../../part2/expectation">Expectation</a>
<a href="../../part2/variance">Variance</a>
<a href="../../part2/binomial">Bernoulli and Binomial</a>
<a href="../../part2/poisson">Poisson Distribution</a>
<a href="../../part2/continuous">Continuous Distribution</a>
<a href="../../part2/normal">Normal Distribution</a>
<a href="../../part2/convolution">Convolution</a>
<a href="../../part2/all_distributions">Random Variable Reference</a>
</li>
</ul>

<!-- part3 -->
<ul class="list-unstyled components">
<li><p href="{pathToLang}part3">Part 3: Probabilistic Models</p></li>
<li>
<a href="../../part3/joint">Joint Probability</a>
<a href="../../part3/independent_vars">Independence in Variables</a>
<a href="../../part3/cond_distributions">Conditional Distributions</a>
<a href="../../part3/variable_bayes">Bayes Theorem Revisited</a>
<a href="../../part3/continuous_joint">Continuous Joint</a>
<a href="../../part3/multivariate_gaussian">Multivariate Gaussian</a>
<a href="../../part3/bayesian_networks">Bayesian Networks</a>
<a href="../../part3/computational_inference">Computational Inference</a>
</li>
</ul>

<!-- part4 -->
<ul class="list-unstyled components">
<li><p href="{pathToLang}part4">Part 4: Uncertainty Theory</p></li>
<li>
<a href="../../part4/clt">Central Limit Theorem</a>
<a href="../../part4/bootstrapping">Bootstrapping</a>
<a href="../../part4/parameters">Uncertainty in Parameters</a>
<a href="../../part4/beta">Beta Distribution</a>
<a href="../../part4/bounds">Probability Bounds</a>
</li>
</ul>

<!-- part5 -->
<ul class="list-unstyled components">
<li><p href="{pathToLang}part5">Part 5: Machine Learning</p></li>
<li>
<a href="../../part5/mle">Maximum Likelihood Estimation</a>
<a href="../../part5/map">Maximum A Posteriori</a>
<a href="../../part5/naive_bayes">Naïve Bayes</a>
<a href="../../part5/optimization">Gradient Ascent Optimization</a>
<a href="../../part5/linear_regression">Linear Regression</a>
<a href="../../part5/log_regression">Logistic Regression</a>
<a href="../../part5/neural_nets">Artificial Neural Networks</a>
</li>
</ul>



    <ul class="list-unstyled components">
        
         <li>
            <p>Part ?: Misfits</p>
        </li>
        <li>
            
             <a href="../../">Sampling from a likelihood function</a>
             <a href="../../">Law of Total Expectation</a>
            
        </li>
    </ul>



    <ul class="list-unstyled CTAs">
        <li>
            Ⓒ Chris Piech, Stanford University
            <!-- <a href="https://compedu.stanford.edu/codeinplace/v1/#/course" class="download">Back to Code in Place</a> -->
        </li>
    </ul>
</nav>
        <!-- Page Content  -->
        <div id="content">

<button type="button" id="sidebarCollapse" class="mobile-only btn btn-dark">
    <i class="fas fa-align-left"></i>
    <span></span>
</button>            
            <div class="container-fluid">
                <div class="row d-flex justify-content-center">
                    <div class="col handout" >
              
                        
 
 <center><h1>Bayes' Theorem</h1></center>
<hr/>

<p>Bayes Theorem is one of the most ubiquitous results in probability for computer scientists. Very often we
know a conditional probability in one direction, say P(E|F), but we would like to know the conditional
probability in the other direction. Bayes Theorem provides a way to convert from one to the other. We can
derive Bayes Theorem by starting with the definition of conditional probability:


$$
\begin{align}
\p(E|F) 
&= \frac{\p(E \and F)}{\p(F)} && \text{Def of conditional probability}  \\
&= \frac{\p(F | E) \cdot \p(E)}{\p(F)} && \text{Substitute the chain rule}
\end{align}
$$

</p>

<p>
<div class="bordered">
	<b><i>Definition</i></b>: Bayes Theorem <br/>

	<p>The most common form of Bayes Theorem is:
		$$
		\p(E|F) = \frac{\p(F | E) \cdot \p(E)}{\p(F)} 
		$$
	</p>

	<p>There are names for the different terms in the Bayes Rule formula. The term $\p(E|F)$  is often called the
"posterior". The term $\p(E)$ is often called the "prior". The term $\p(F|E)$ is called the update and $\p(F)$ is
often called the normalization constant.</p>

<p>There are several techniques for handling the case where the denominator is not know. One technique is to use the law of total probability to expand out the term, resulting in another formula, also called Bayes' Theorem:
$$
\p(E|F) = \frac{\p(F | E) \cdot \p(E)}{\p(F|E)\cdot \p(E) + \p(F|E\c) \cdot \p(E\c)} 
$$
</p>
Recall the law of total probability which is responsible for our new denominator:
$$
\begin{align}
\p(F) &= \p(F \and E) + \p(F \and E\c)\\
&= \p(F|E)\cdot \p(E) + \p(F|E\c) \cdot \p(E\c)
\end{align}
$$
</div>
</p>

<p>A common scenario for applying the Bayes Rule formula is when you want to know the probability of
something “unobservable” given an “observed” event. For example, you want to know the probability that a
student understands a concept, given that you observed them solving a particular problem. It turns out it is
much easier to first estimate the probability that a student can solve a problem given that they understand the
concept and then to apply Bayes Theorem. Intuitively, you can think about this as updating a belief given
evidence.</p>

<h2>Unknown normalization constant</h2>

<p>There are times when we would like to use Bayes Theorem to update a belief, but there is no way to calculate
the probability of the event observed, P(F). All hope is not lost. In such situations we can still calculate
the relative probability of events. For example, imagine we would like to answer the question, is even A
or event B more likely given an observation F. We can express this mathematically as calculating whether
P(A|F)/P(B|F) is greater than or equal to 1. Both of those terms can be expanded using Bayes, and when
they are expanded the P(F) term cancels out:</p>

               


<script type="text/javascript">
    $(document).ready(function () {
        $("#sidebar").mCustomScrollbar({
            theme: "minimal",
            mouseWheel: {preventDefault: true},
            scrollInertia: 60
        });

        $('#sidebarCollapse').on('click', function () {
            $('#sidebar, #content').toggleClass('active');
            $('.collapse.in').toggleClass('in');
            $('a[aria-expanded=true]').attr('aria-expanded', 'false');
        });
    });
</script>                
                    </div>
                </div>
            </div>
        </div>
    </div>

</body>


</html>